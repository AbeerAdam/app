# -*- coding: utf-8 -*-
"""SearchQueryAndReturnHadith.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M4YoHdNt4RO3W_yKj85eInQ3L3FISBPk
"""
from doctest import master

import arabic as arabic
from nltk.downloader import unzip



import pandas as pd
data = pd.read_excel("KeyWordsDataset.xlsx")
cols =data.columns
print(cols)
df=pd.read_csv(r"C:/Users/ab00e/Desktop/tkprojict/build/arabic-stop-words-master/arabic-stop-words-master/list.txt",)
df.columns=["stopWords"]

keyWords= data['KeyWords']
ids= data['hadith-id']

import re
def noramlize(text):
    text = re.sub(r"[إأٱآا]", "ا", text)
    text = re.sub(r"ى", "ي", text)
    text = re.sub(r"ؤ", "ء", text)
    text = re.sub(r"ئ", "ء", text)
    text = re.sub(r'[^ا-ي ]', "", text)

    noise = re.compile(""" ّ    | # Tashdid
                         َ    | # Fatha
                         ً    | # Tanwin Fath
                         ُ    | # Damma
                         ٌ    | # Tanwin Damm
                         ِ    | # Kasra
                         ٍ    | # Tanwin Kasr
                         ْ    | # Sukun
                         ـ     # Tatwil/Kashida
                     """, re.VERBOSE)
    text = re.sub(noise, '', text)
    return text

extraList=["رسول",'الله',"صلى","عليه","وسلم","أن","يقول","سلم","قال","النَّبِيَّ","من","إلى","عن","على","في","و","صلي","النبي","فقولوا","رضي"]
extraStopWords=pd.DataFrame(extraList,columns=['stopWords'])
noramlizedStopWords=[]
for t in df["stopWords"]:
  noramlizedStopWords.append(noramlize(t))
for i in range(len(extraList)):
  noramlizedStopWords.append(extraList[i])
noramlizedStopWords

import nltk
nltk.download('punkt')

import nltk.tokenize as nt
def stopWordRmove(text , stop_words):
    needed_words = []
    words = nt.word_tokenize(text)
    print("Words",words)
    for w in words:
        if w not in (stop_words):
            needed_words.append(w)
    filtered_sentence = " ".join(needed_words)
    return filtered_sentence

query = input("get query : ")
normQuery = noramlize(query)
processedQuery= stopWordRmove(normQuery,noramlizedStopWords)
print(processedQuery)
terms =processedQuery.split(" ")
idsList= []
for index,t in enumerate(data['KeyWords']):
  for ind,term in enumerate(terms):
    if term in t.split(" "):
          idsList.append(index+1)

  print(idsList)







